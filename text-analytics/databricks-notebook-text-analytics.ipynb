{"cells":[{"cell_type":"markdown","source":["<h1> About this notebook </h1>\n\nThis notebook has been created to demo capabilities of Databricks notebooks and Azure Cognitive Services entity recognition API.\n\nFollowing Azure services have been used in this demo:\n1. Azure Data Lake Storage (ADLS Gen2) - used for storing files that have to be processed for PII detection.\n2. Azure Databricks - one stop shop to run our program code; transform data; interact with other Azure services to deliver a complete end to end solution.\n3. Azure SQL DB - used as a metadata store where Databricks writes information like job start/end time; file name that is being processed etc.\n4. Azure Cosmos DB - store output of Azure Cognitive Services in case PII is detected. Where no PII is detected, the output is ignored. \n5. Azure Cognitive Services / Text Analytics API -  (Named Entity Recognition) is used for detecting PII.\n</br>\n\n\n\n\n**To run this notebook in your subscription, please replace text in <> with your account keys/credentials. **"],"metadata":{}},{"cell_type":"markdown","source":["<h4> Install Python packages required for this notebook </h4>\n\n\nInstall packages for Python required to run this notebook. \n\n** You only need to run this step once when you run this notebook for the first time. **"],"metadata":{}},{"cell_type":"code","source":["# Install packages\n# Note - installation of packages can be automated \n\ndbutils.library.installPyPI(\"requests\")\ndbutils.library.installPyPI(\"simpleJson\")\ndbutils.library.installPyPI(\"pyodbc\")\ndbutils.library.installPyPI(\"azure.cosmos\")\ndbutils.library.installPyPI(\"requests\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">PyPI package azure.cosmos has been installed already. The previous installed package is PyPI:(azure.cosmos)-(empty)-(empty)-(empty). To resolve this issue detach and retach the notebook to create a new environment or rename the package.\nOut[20]: False</div>"]}}],"execution_count":3},{"cell_type":"markdown","source":["<h4> Install ODBC drivers for Microsoft SQL Server on Databricks cluster </h4>\n\nWe are using PYODBC package to connect to Azure SQL DB. This step installs the SQL Server driver.\n\n** You only need to run this step once when you run this notebook for the first time. **"],"metadata":{}},{"cell_type":"code","source":["%sh\n\ncurl https://packages.microsoft.com/keys/microsoft.asc | apt-key add -\ncurl https://packages.microsoft.com/config/ubuntu/16.04/prod.list > /etc/apt/sources.list.d/mssql-release.list \napt-get update\nACCEPT_EULA=Y apt-get install msodbcsql17\napt-get -y install unixodbc-dev"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100   983  100   983    0     0   1226      0 --:--:-- --:--:-- --:--:--  1225\n100   983  100   983    0     0   1226      0 --:--:-- --:--:-- --:--:--  1225\nOK\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100    79  100    79    0     0    144      0 --:--:-- --:--:-- --:--:--   144\nGet:1 https://packages.microsoft.com/ubuntu/16.04/prod xenial InRelease [4,003 B]\nGet:2 https://cran.rstudio.com/bin/linux/ubuntu xenial-cran35// InRelease [3,625 B]\nGet:3 http://security.ubuntu.com/ubuntu xenial-security InRelease [109 kB]\nGet:4 https://packages.microsoft.com/ubuntu/16.04/prod xenial/main amd64 Packages [120 kB]\nHit:5 http://archive.ubuntu.com/ubuntu xenial InRelease\nGet:6 https://cran.rstudio.com/bin/linux/ubuntu xenial-cran35// Packages [78.7 kB]\nGet:7 http://archive.ubuntu.com/ubuntu xenial-updates InRelease [109 kB]\nGet:8 http://security.ubuntu.com/ubuntu xenial-security/main amd64 Packages [1,003 kB]\nGet:9 http://archive.ubuntu.com/ubuntu xenial-backports InRelease [107 kB]\nGet:10 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 Packages [1,381 kB]\nGet:11 http://security.ubuntu.com/ubuntu xenial-security/universe amd64 Packages [591 kB]\nGet:12 http://archive.ubuntu.com/ubuntu xenial-updates/universe amd64 Packages [995 kB]\nGet:13 http://security.ubuntu.com/ubuntu xenial-security/multiverse amd64 Packages [6,283 B]\nGet:14 http://archive.ubuntu.com/ubuntu xenial-updates/multiverse amd64 Packages [19.3 kB]\nFetched 4,528 kB in 9s (502 kB/s)\nReading package lists...\nReading package lists...\nBuilding dependency tree...\nReading state information...\nThe following package was automatically installed and is no longer required:\n  libgnutls-openssl27\nUse &#39;sudo apt autoremove&#39; to remove it.\nThe following NEW packages will be installed:\n  msodbcsql17\n0 upgraded, 1 newly installed, 0 to remove and 60 not upgraded.\nNeed to get 750 kB of archives.\nAfter this operation, 0 B of additional disk space will be used.\nGet:1 https://packages.microsoft.com/ubuntu/16.04/prod xenial/main amd64 msodbcsql17 amd64 17.4.2.1-1 [750 kB]\ndebconf: delaying package configuration, since apt-utils is not installed\nFetched 750 kB in 0s (751 kB/s)\nSelecting previously unselected package msodbcsql17.\n(Reading database ... \n(Reading database ... 5%\n(Reading database ... 10%\n(Reading database ... 15%\n(Reading database ... 20%\n(Reading database ... 25%\n(Reading database ... 30%\n(Reading database ... 35%\n(Reading database ... 40%\n(Reading database ... 45%\n(Reading database ... 50%\n(Reading database ... 55%\n(Reading database ... 60%\n(Reading database ... 65%\n(Reading database ... 70%\n(Reading database ... 75%\n(Reading database ... 80%\n(Reading database ... 85%\n(Reading database ... 90%\n(Reading database ... 95%\n(Reading database ... 100%\n(Reading database ... 61911 files and directories currently installed.)\nPreparing to unpack .../msodbcsql17_17.4.2.1-1_amd64.deb ...\ndebconf: unable to initialize frontend: Dialog\ndebconf: (Dialog frontend will not work on a dumb terminal, an emacs shell buffer, or without a controlling terminal.)\ndebconf: falling back to frontend: Readline\nUnpacking msodbcsql17 (17.4.2.1-1) ...\nSetting up msodbcsql17 (17.4.2.1-1) ...\nReading package lists...\nBuilding dependency tree...\nReading state information...\nThe following package was automatically installed and is no longer required:\n  libgnutls-openssl27\nUse &#39;sudo apt autoremove&#39; to remove it.\nThe following additional packages will be installed:\n  libodbc1 odbcinst odbcinst1debian2 unixodbc\nSuggested packages:\n  unixodbc-bin\nThe following packages will be upgraded:\n  libodbc1 odbcinst odbcinst1debian2 unixodbc unixodbc-dev\n5 upgraded, 0 newly installed, 0 to remove and 55 not upgraded.\nNeed to get 714 kB of archives.\nAfter this operation, 104 kB of additional disk space will be used.\nGet:1 https://packages.microsoft.com/ubuntu/16.04/prod xenial/main amd64 libodbc1 amd64 2.3.7 [511 kB]\nGet:2 https://packages.microsoft.com/ubuntu/16.04/prod xenial/main amd64 unixodbc-dev amd64 2.3.7 [37.1 kB]\nGet:3 https://packages.microsoft.com/ubuntu/16.04/prod xenial/main amd64 unixodbc amd64 2.3.7 [19.6 kB]\nGet:4 https://packages.microsoft.com/ubuntu/16.04/prod xenial/main amd64 odbcinst amd64 2.3.7 [12.0 kB]\nGet:5 https://packages.microsoft.com/ubuntu/16.04/prod xenial/main amd64 odbcinst1debian2 amd64 2.3.7 [135 kB]\ndebconf: delaying package configuration, since apt-utils is not installed\nFetched 714 kB in 1s (526 kB/s)\n(Reading database ... \n(Reading database ... 5%\n(Reading database ... 10%\n(Reading database ... 15%\n(Reading database ... 20%\n(Reading database ... 25%\n(Reading database ... 30%\n(Reading database ... 35%\n(Reading database ... 40%\n(Reading database ... 45%\n(Reading database ... 50%\n(Reading database ... 55%\n(Reading database ... 60%\n(Reading database ... 65%\n(Reading database ... 70%\n(Reading database ... 75%\n(Reading database ... 80%\n(Reading database ... 85%\n(Reading database ... 90%\n(Reading database ... 95%\n(Reading database ... 100%\n(Reading database ... 61927 files and directories currently installed.)\nPreparing to unpack .../libodbc1_2.3.7_amd64.deb ...\nUnpacking libodbc1:amd64 (2.3.7) over (2.3.1-4.1) ...\nPreparing to unpack .../unixodbc-dev_2.3.7_amd64.deb ...\nUnpacking unixodbc-dev (2.3.7) over (2.3.1-4.1) ...\nPreparing to unpack .../unixodbc_2.3.7_amd64.deb ...\nUnpacking unixodbc (2.3.7) over (2.3.1-4.1) ...\nPreparing to unpack .../odbcinst_2.3.7_amd64.deb ...\nUnpacking odbcinst (2.3.7) over (2.3.1-4.1) ...\nPreparing to unpack .../odbcinst1debian2_2.3.7_amd64.deb ...\nUnpacking odbcinst1debian2:amd64 (2.3.7) over (2.3.1-4.1) ...\nProcessing triggers for libc-bin (2.23-0ubuntu11) ...\nProcessing triggers for man-db (2.7.5-1) ...\nSetting up libodbc1:amd64 (2.3.7) ...\nSetting up odbcinst (2.3.7) ...\nSetting up odbcinst1debian2:amd64 (2.3.7) ...\nSetting up unixodbc (2.3.7) ...\nSetting up unixodbc-dev (2.3.7) ...\nProcessing triggers for libc-bin (2.23-0ubuntu11) ...\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["# Import modules required to run demo code in this notebook\nimport os \nimport requests\nimport simplejson as json\nimport io\nimport datetime\nimport pyodbc\nimport azure.cosmos.cosmos_client as cosmos_client\nimport azure.cosmos.errors as errors\nimport azure.cosmos.http_constants as http_constants\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["# Some functions to create batch ID and date/timestamp string that we will use to tag the workloads\n\ndef dtStr():\n    x=datetime.datetime.now()\n    dtTime=x.strftime(\"%Y\")+\"-\"+x.strftime(\"%m\")+\"-\"+x.strftime(\"%d\")+\" \"+x.strftime(\"%H\")+\":\"+x.strftime(\"%M\")+\":\"+x.strftime(\"%S\")\n    return dtTime\n\ndef batchStr():\n    # create a unique batchid to identify workload\n    x = datetime.datetime.now()\n    batchId = x.strftime(\"%Y%m%d%H%M%S%f\")\n    return batchId\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"code","source":["# Job ID - unique identifier to tag each workload\n\nbatchId=batchStr()\nprint(\"Batch ID for this workload is \"+batchId)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Batch ID for this workload is 20191124024716025452\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["# Set variables for Text Analytics API\n# Please note, for prod implementation please include reference to Azure Key Vault instead of storing passwords/keys in plain text inside this notebook\napiEndpoint = \"https://eastus.cognitiveservices.azure.com/text/analytics/v3.0-preview.1/entities/recognition/pii\"\nsubKey = <Azure Text Analytics API key>\nlanguage = 'en'\nid = 1"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"code","source":["# For this demo, we have pre-configured an ADLS instance and copied a few demo files there\n\ndataLakeAccountDNS = \"Replace this with your Storage Account name\" + \".blob.core.windows.net\"\ndatalakeAccountKey = \"Replace this with your Storage Key\"\ndataLakeConf = \"fs.azure.account.key.\" + dataLakeAccountDNS\ndataLakeExtraConfig = {dataLakeConf:datalakeAccountKey}\ndataLakeContainer = \"Replace this with your container name\"\nrawContainer = \"wasbs://\"+dataLakeContainer+\"@\" + dataLakeAccountDNS\n\n# This is the mountpoint that will be used by Databricks to access files in ADLS \nrawMountPoint = \"/mnt/raw\"\ndbutils.fs.mount(source = rawContainer, mount_point = rawMountPoint, extra_configs = dataLakeExtraConfig)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">ExecutionError</span>                            Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-242315534026664&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      7</span> rawContainer <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">&#34;wasbs://adlsgen2storage@&#34;</span> <span class=\"ansi-blue-fg\">+</span> dataLakeAccountDNS\n<span class=\"ansi-green-intense-fg ansi-bold\">      8</span> rawMountPoint <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">&#34;/mnt/raw&#34;</span>\n<span class=\"ansi-green-fg\">----&gt; 9</span><span class=\"ansi-red-fg\"> </span>dbutils<span class=\"ansi-blue-fg\">.</span>fs<span class=\"ansi-blue-fg\">.</span>mount<span class=\"ansi-blue-fg\">(</span>source <span class=\"ansi-blue-fg\">=</span> rawContainer<span class=\"ansi-blue-fg\">,</span> mount_point <span class=\"ansi-blue-fg\">=</span> rawMountPoint<span class=\"ansi-blue-fg\">,</span> extra_configs <span class=\"ansi-blue-fg\">=</span> dataLakeExtraConfig<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/local_disk0/tmp/1574563385388-0/dbutils.py</span> in <span class=\"ansi-cyan-fg\">f_with_exception_handling</span><span class=\"ansi-blue-fg\">(*args, **kwargs)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    312</span>                     exc<span class=\"ansi-blue-fg\">.</span>__context__ <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    313</span>                     exc<span class=\"ansi-blue-fg\">.</span>__cause__ <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-fg\">--&gt; 314</span><span class=\"ansi-red-fg\">                     </span><span class=\"ansi-green-fg\">raise</span> exc\n<span class=\"ansi-green-intense-fg ansi-bold\">    315</span>             <span class=\"ansi-green-fg\">return</span> f_with_exception_handling\n<span class=\"ansi-green-intense-fg ansi-bold\">    316</span> \n\n<span class=\"ansi-red-fg\">ExecutionError</span>: An error occurred while calling o337.mount.\n: java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/raw; nested exception is: \n\tjava.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/raw\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:123)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:63)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.mount(DBUtilsCore.scala:465)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/raw\n\tat scala.Predef$.require(Predef.scala:281)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$insertMount$1(MetadataManager.scala:205)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.withRetries(MetadataManager.scala:307)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.insertMount(MetadataManager.scala:201)\n\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:79)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:103)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:102)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:102)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$3.applyOrElse(DbfsServerBackend.scala:298)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$3.applyOrElse(DbfsServerBackend.scala:276)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$2(ServerBackend.scala:50)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:67)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:67)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:46)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:417)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:239)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:234)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:231)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:13)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:276)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:269)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:13)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:398)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:337)\n\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:13)\n\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:45)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleRPC$2(JettyServer.scala:509)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:509)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:419)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$4(JettyServer.scala:277)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:239)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:234)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:231)\n\tat com.databricks.rpc.JettyServer$.withAttributionContext(JettyServer.scala:153)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:276)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:269)\n\tat com.databricks.rpc.JettyServer$.withAttributionTags(JettyServer.scala:153)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:268)\n\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:183)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:707)\n\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:585)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:515)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:539)\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:333)\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)\n\tat org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\n\t... 1 more\n</div>"]}}],"execution_count":10},{"cell_type":"code","source":["# List files on ADLS mount \n# These are the files that we will be processing\n\n# Full pathname - mount + directory \ninDir=\"/dbfs/mnt/raw/inputfiles\"\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":11},{"cell_type":"code","source":["# Azure SQLDB details. We are using this to log job details.\n# For production implementation, please do not store database password in clear text.\n# Note - Please add Databricks cluster IP address to firewall rules \n\ndbServerName = \"Replace this with your instance of Azure SQL DB\"\ndbName = \"Replace this with the database name\"\ndbUser = \"Replace this with the database user\"\ndbPass = \"Replace this password for database account\"\ndriver= 'ODBC Driver 17 for SQL Server'\ncnxn = pyodbc.connect('DRIVER='+driver+';SERVER='+dbServerName+';PORT=1433;DATABASE='+dbName+';UID='+dbUser+';PWD='+dbPass)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":12},{"cell_type":"code","source":["# Azure Cosmos DB is used for storing output of Text Analytics where PII is detected.\n# For this demo, we deployed an instance of Azure Cosmos DB separately. \n# Please do not store keys in clear text inside a notebook for production purposes. Please use Azure Key Vault for storing keys.\n\nurl=\"Replace this with Azure Cosmos DB URL\"\nkey=\"Replace this with Azure Cosmos DB key\"\ndatabase_id = \"Replace this with Cosmos database name\"\ncollection_id = \"Replace this with Cosmos container\"\ndatabase_link = 'dbs/' + database_id\ncollection_link = database_link + '/colls/' + collection_id\nclient=cosmos_client.CosmosClient(url,{'masterKey': key})"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":13},{"cell_type":"markdown","source":["<h4> This part of the notebook loops through all the files that are stored on ADLS Gen2 directory which we mounted to Databricks cluster.</h4>\nFor each file processed as part of this step, Databricks creates an entry in Azure SQL DB to mark start and completion of a job. \nThis step also calls Text Analytics API and captures the output of the API call and stores it into Cosmos DB if PII is detected. \n\n**Note on whether or not to batch input datasets that have to be parsed by Text Analytics API**<br/>\nUsers have option to either batch data from all the files and call Text Analytics API once or call the API once per file. \nFor this demo, the code makes multiple calls to the API (once per each input file). <br/>"],"metadata":{}},{"cell_type":"code","source":["\n\ninDir=\"/dbfs/mnt/raw/inputfiles\"\n# loop through directory and append content of each file as an array element. Increment ID and keep appending records until you end up with a single Json document\nfor file in os.listdir(inDir):\n    # initialise id counter\n    strObj = \" \"\n    \n    # read contents of the file and store it in a variable\n    fullFileName=inDir+\"/\"+file \n    readFile = open(inDir+\"/\"+file,\"r\")\n    \n    print(\"Processing file .. \"+fullFileName)\n\n    # Insert job status in Azure SQL DB when file processing starts\n    # writing to azure sqldb\n    stDtTime=dtStr()\n    insertStr=\"insert into dbo.joblogs values ('\"+batchId+\"','\"+fullFileName+\"','Started','\"+stDtTime+\"')\"\n    cursor = cnxn.cursor()\n    cursor.execute(insertStr)\n    cnxn.commit()\n\n    for line in readFile:\n        \n        # read lines into a string object \n        strObj = strObj + line\n\n    # create JSON structure that can be processed by Text Analytics API\n    jsonDoc = { \"documents\": [{\"language\":language,\"id\":id,\"Text\":strObj}]}\n\n    # call API \n    headers = {\"Ocp-Apim-Subscription-Key\": subKey}\n    response = requests.post(apiEndpoint, headers=headers, json=jsonDoc)\n    entities = response.json()\n \n    # print(json.dumps(entities, separators=(',', ':'), sort_keys=True, indent = 4 * ' '))\n\n    # If PII is detected by the API - write output to Cosmos DB container else ignore the entry\n    if len(entities['documents'][0]['entities']) != 0:\n\n        print(\"potential pii present\")\n\n        # Write API output to Cosmos DB collection using SQL API\n        print('Writing API results to Cosmos DB ... ')\n        # Add batch ID and filename to the JSON document before it gets written to Cosmos DB\n        entities.__setitem__('batchid',batchId)\n        entities.__setitem__('input data',fullFileName)\n        \n        # Write document to Cosmos DB collection\n        client.CreateItem(collection_link,entities)\n        \n        # Insert job status into Azure SQL DB\n        stDtTime=dtStr()\n        insertStr=\"insert into dbo.joblogs values ('\"+batchId+\"','\"+fullFileName+\"','Completed','\"+stDtTime+\"')\"\n        cursor = cnxn.cursor()\n        cursor.execute(insertStr)\n        cnxn.commit()\n  \n    else:\n        # Insert job status into Azure SQL DB\n        stDtTime=dtStr()\n        insertStr=\"insert into dbo.joblogs values ('\"+batchId+\"','\"+fullFileName+\"','Completed','\"+stDtTime+\"')\"\n        cursor = cnxn.cursor()\n        cursor.execute(insertStr)\n        cnxn.commit()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Processing file .. /dbfs/mnt/raw/inputfiles/demo.txt\npotential pii present\nWriting API results to Cosmos DB ... \nProcessing file .. /dbfs/mnt/raw/inputfiles/file1.txt\npotential pii present\nWriting API results to Cosmos DB ... \nProcessing file .. /dbfs/mnt/raw/inputfiles/file2.txt\npotential pii present\nWriting API results to Cosmos DB ... \nProcessing file .. /dbfs/mnt/raw/inputfiles/file3.txt\npotential pii present\nWriting API results to Cosmos DB ... \nProcessing file .. /dbfs/mnt/raw/inputfiles/file4.txt\npotential pii present\nWriting API results to Cosmos DB ... \nProcessing file .. /dbfs/mnt/raw/inputfiles/input.json\npotential pii present\nWriting API results to Cosmos DB ... \nProcessing file .. /dbfs/mnt/raw/inputfiles/pii.txt\npotential pii present\nWriting API results to Cosmos DB ... \nProcessing file .. /dbfs/mnt/raw/inputfiles/testfile.txt\npotential pii present\nWriting API results to Cosmos DB ... \n</div>"]}}],"execution_count":15}],"metadata":{"name":"databricks-notebook-text-analytics-v9","notebookId":1670903862187756},"nbformat":4,"nbformat_minor":0}
